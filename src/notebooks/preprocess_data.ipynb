{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "import sys\n",
                "sys.path.append("../src/")\n",
                "from utils.load_data import load_data\n",
                "from utils.save_data import save_data\n",
                "\n",
                "def preprocess_full_channel_data(data):\n",
                "    # Handling missing values\n",
                "    data.fillna({\"remaining_size\": 0, \"price\": data[\"price\"].mean()}, inplace=True)\n",
                "    \n",
                "    # Feature Engineering\n",
                "    # Create a new feature that indicates the change in remaining size\n",
                "    data[\"remaining_size_change\"] = data.groupby(\"order_id\")[\"remaining_size\"].diff().fillna(0)\n",
                "    \n",
                "    # Normalize numeric columns\n",
                "    numeric_features = [\"price\", \"size\", \"remaining_size\", \"remaining_size_change\"]\n",
                "    numeric_transformer = Pipeline(steps=[\n",
                "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
                "        (\"scaler\", MinMaxScaler())])\n",
                "    \n",
                "    # Encode categorical variables\n",
                "    categorical_features = [\"type\", \"side\", \"reason\"]\n",
                "    categorical_transformer = Pipeline(steps=[\n",
                "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
                "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
                "    \n",
                "    # Column transformer for applying transformations to specific columns\n",
                "    preprocessor = ColumnTransformer(\n",
                "        transformers=[\n",
                "            (\"num\", numeric_transformer, numeric_features),\n",
                "            (\"cat\", categorical_transformer, categorical_features)])\n",
                "\n",
                "    data_processed = pd.DataFrame(preprocessor.fit_transform(data),\n",
                "                                  columns=numeric_features + list(preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names_out(categorical_features)))\n",
                "    \n",
                "    return data_processed\n",
                "\n",
                "def preprocess_ticker_data(data):\n",
                "    # Drop unnecessary columns\n",
                "    data = data.drop(columns=["type", "product_id"])\n",
                "\n",
                "    # Normalize numeric columns\n",
                "    scaler = MinMaxScaler()\n",
                "    numeric_cols = [\"price\", \"open_24h\", \"volume_24h\", \"low_24h\", \"high_24h\", \"volume_30d\", \"best_bid\", \"best_ask\", \"last_size\"]\n",
                "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
                "    \n",
                "    # One-hot encode "side"\n",
                "    data = pd.get_dummies(data, columns=[\"side\"])\n",
                "    \n",
                "    return data\n",
                "\n",
                "def preprocess_data():\n",
                "    full_channel, ticker = load_data()\n",
                "    \n",
                "    # Preprocess each dataset\n",
                "    full_channel_processed = preprocess_full_channel_data(full_channel)\n",
                "    ticker_processed = preprocess_ticker_data(ticker)\n",
                "    \n",
                "    # Save processed data\n",
                "    save_data(full_channel_processed, \"../data/processed/full_channel_processed.csv\")\n",
                "    save_data(ticker_processed, \"../data/processed/ticker_processed.csv\")\n",
                "    print(\"Data preprocessing complete and files saved.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-04-30 15:20:10,145 - INFO - Loaded FullChannel data with 1774678 rows and 18 columns.\n",
                        "2024-04-30 15:20:10,146 - INFO - Loaded Ticker data with 102805 rows and 15 columns.\n",
                        "2024-04-30 15:20:23,626 - INFO - Data successfully saved to ../data/processed/full_channel_processed.csv\n",
                        "2024-04-30 15:20:24,357 - INFO - Data successfully saved to ../data/processed/ticker_processed.csv\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data preprocessing complete and files saved.\n"
                    ]
                }
            ],
            "source": [
                "preprocess_data()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "spoof",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}